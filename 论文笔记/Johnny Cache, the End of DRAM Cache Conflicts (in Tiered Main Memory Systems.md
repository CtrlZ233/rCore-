
## Background

在PMEM背景下，Intel  Optan提供的PMEM的两种模式，其中一种模式是缓存模式，即DRAM完全作为PMEM的缓存，由硬件直接管理DRAM缓存，类似于'L4 cache'。

硬件管理的DRAM缓存采用的是直接映射法。当发生冲突时会将旧的cache line驱逐出DRAM并写回PMEM。

以往的工作（21年的HeMem）认为硬件实现分层系统效率低下，因为硬件缺乏应用程序需求的高层视图，因为硬件的缓存策略必须保持简单。因此这些工作使用软件来管理缓存，由操作系统决定哪些页面应该被缓存到DRAM cache中。内核通常在DRAM中分配尽可能多的页面，并且当 DRAM 已满，后续页面将分配在慢速层（PMEM）中。 守护进程负责迁移经常访问的页面 （热页）从慢速层到 DRAM，并且不频繁 从 DRAM 访问慢速层的页面（冷页面）。 旨在提高推断热点页面集高精度和降低开销。

作者通过实验发现，硬件管理缓存会导致大量的缓存冲突，而冲突的代价会非常的大。

![](../image/JC_confict.png)

![](../image/JC_performance.png)
## Design

文章的设计基于这样的理念：只要缓存中的冲突很少，DRAM 缓存就是高效的。

之前的硬件管理之所以低效，是因为常常容易发生大量的缓存冲突，既然硬件缺乏应用程序需求的高层视图，那作者就从软件进行改进，用软件配合硬件。

具体到操作系统，之前的Linux在分配页框的时候基本上时接近随机分配，随机分配的页框容易在DRAM cache里遭受到生日悖论。因此作者在内核中维护了缓存槽的负载情况，并实时根据缓存槽的负载情况来分配物理页框，依次来减少冲突。


## Personal Thinking

1. 这里对页面分配算法的改进，对非PMEM情况下的普通CPU 缓存是否有效呢？
	1. 这里在内核中维护缓存槽的新增开销是否会抵消掉缓解cache冲突减少的开销。

